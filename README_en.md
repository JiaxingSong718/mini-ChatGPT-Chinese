<div align="center">
# A Small Chat with 0.1B Chinese Language Model: mini-ChatGPT-Chinese-0.1B  

English | [Chinese](./README.md) 

</div>


# I.ğŸ‘‹Introduction 
The parameters of modern large language models are often quite large, making inference slow on consumer-grade computers, let alone training a model from scratch. The goal of this project is to train a generative language model from 0, including tokenizer training, model pre-training, SFT instruction fine-tuning, RLHF optimization (DPO), etc.

**mini-ChatGPT-Chinese** is a small Chinese dialogue model with only 0.1B parameters (around 105M including shared weights).

- Openly disclose all sources of pre-training, SFT instruction fine-tuning, and DPO preference optimization datasets.
- Support stopping and resuming training at any point during the process.
- Pre-training: integrate into end-to-end pre-training.
- SFT fine-tuning.
- RLHF preference optimization: use DPO for full preference optimization (to be updated~).

# II.ğŸ› ï¸mini-ChatGPT-Chinese-0.1B model training process 

## 2.1 Pre-training Datasets

All datasets are from publicly available **single-turn dialogue** datasets. The main datasets include:

1. Community Q&A json version webtext2019zh - large-scale, high-quality dataset, see: [nlp_chinese_corpus](https://github.com/brightmart/nlp_chinese_corpus). A total of 4.1 million entries, approximately 2 million used for tokenizer training.
2. baike_qa2019 Baidu Encyclopedia Q&A, see: <https://aistudio.baidu.com/datasetdetail/107726>, a total of 1.4 million entries, approximately 200,000 selected.
3. Chinese medical field Q&A dataset, see: [Chinese-medical-dialogue-data](https://github.com/Toyhom/Chinese-medical-dialogue-data), around 790,000 entries.
4. Zhihu Q&A data, see: [Zhihu-KOL](https://huggingface.co/datasets/wangrui6/Zhihu-KOL), around 1 million entries.
5. Instruction training data from Belle's open-source project, see: [train_0.5M_CN](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN), around 500,000 entries.

The total dataset size is approximately 4.5 million entries: pre-training set: around 2 million, evaluation set: not yet set. SFT fine-tuning data is around 500,000, and the DPO optimization dataset is to be updated.

## 2.2 Model

GPT model (Generative Pre-Trained Transformer), for details, see the paper: [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165).

The model configuration can be found in [config.py](https://github.com/JiaxingSong718/mini-ChatGPT-Chinese/blob/main/config/config.py), official `GPT-3 Small`: `decoder layer` consists of 12 layers.

Model parameters: 0.1B. Vocabulary size: 13,000, including only Chinese and a small amount of English.

## 2.3 Training Process

Hardware:

```bash
# Pre-training and SFT phase:
CPU: Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz
Memory: 112 GB
GPU: NVIDIA 50%A100(40GB) * 1
```

1. **Tokenizer Training**: Existing `tokenizer` training libraries encountered OOM issues with large datasets, so the entire corpus was merged and a vocabulary was constructed using a method similar to `BPE` based on word frequency. The process took two days.

2. **Pre-training**: Dynamic learning rate from `1e-4` to `5e-3`, pre-training took 4 days.

3. **Prompt Supervised Fine-tuning (SFT)**: Using the `belle` instruction training dataset, with a dynamic learning rate from `1e-7` to `5e-5`, fine-tuning took 1 day.

4. **DPO Direct Preference Optimization (RLHF)**: To be updated.

## 2.4 Conversation Effect Demonstration (DPO not yet implemented)

```
python client.py
```

```
User> ä½ å¥½
Response>  <|im_start|>system
You are a helpful assistant.
<|im_end|>
<|im_start|>user
ä½ å¥½
<|im_end|>
<|im_start|>assistant
ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ

User> ç°åœ¨å¤–é¢å¤©æ°”æ€ä¹ˆæ ·
Response<  <|im_start|>system
You are a helpful assistant.
<|im_end|>
<|im_start|>user
ç°åœ¨å¤–é¢å¤©æ°”æ€ä¹ˆæ ·
<|im_end|>
<|im_start|>assistant
æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”å®æ—¶å¤©æ°”é—®é¢˜ï¼Œå»ºè®®æ‚¨æŸ¥è¯¢å½“åœ°çš„å¤©æ°”é¢„æŠ¥ã€‚

User> ä¸­å›½çš„äº”å¤§åæ¹–
Response<  <|im_start|>system
You are a helpful assistant.
<|im_end|>
<|im_start|>user
ä¸­å›½çš„äº”å¤§åæ¹–
<|im_end|>
<|im_start|>assistant
ä¸­å›½çš„äº”å¤§åæ¹–æœ‰å¤ªæ¹–ã€æ´åº­æ¹–ã€é„±é˜³æ¹–ã€æ´ªæ³½æ¹–ã€éº»æ¹–ã€‚
```

There are issues: the pre-training dataset contains only around 2 million entries, and the model parameters are just 0.1B, which cannot cover all domains. This may result in instances of irrelevant answers and generate meaningless content.

# III. ğŸ“‘ Usage Instructions

## 3.1 Starting from Cloning the Repository

> [!CAUTION]
> In the `prompt`, `response`, and other fields during the pre-training, SFT, and RLHF phases, be sure to include the `[EOS]` sequence end marker.

### 3.1.1 Clone the Project:

```bash
git clone https://github.com/JiaxingSong718/mini-ChatGPT-Chinese.git

cd mini-ChatGPT-Chinese
```

### 3.1.2 Install Dependencies

This project recommends using `python 3.10`; older Python versions may not be compatible with the required third-party libraries.

Install via pip:

```bash
pip install -r ./requirements.txt
```

## 3.2 Tokenizer Training

**1. Prepare the txt Corpus**

This project primarily uses the Chinese Wikipedia. To obtain the Chinese Wikipedia corpus, download the Chinese Wiki from the following link: [zhwiki](https://dumps.wikimedia.org/zhwiki/). Download the `zhwiki-[archive date]-pages-articles-multistream.xml.bz2` file, which is approximately 2.7GB. Convert the downloaded bz2 file to `wiki.txt` using [WikiExtractor](https://github.com/apertium/WikiExtractor), and then use Python's `OpenCC` library to convert it to Simplified Chinese. Finally, place the resulting `wiki.simple.txt` file in the `data` directory at the root of the project.

Training the tokenizer is memory-intensive. If your corpus is very large (the merged `txt` file exceeds 2GB), it is recommended to sample the corpus by category or proportion to reduce training time and memory consumption.

**2.Train the Tokenizer**

```
# Ensure your training corpus `txt` file is in the data directory
cd Tokenizer
python train_tokenizer.py
```

The result of training a tokenizer with a vocabulary size of 13,000:
```python
from Tokenizer import BPETokenizer

# åŠ è½½
tokenizer = BPETokenizer()
tokenizer.load('./tokenizer.bin')
idx,tokens,text = tokenizer.encode('<|im_start|>user\nç»™å®šä¸€ä¸ªè‹±æ–‡å¥å­ï¼Œç¿»è¯‘æˆä¸­æ–‡\n<|im_end|>')
print('encode: ',idx)
print('===================================================================')
print('token: ',tokens)
print('===================================================================')
print('text: ',text)
print('===================================================================')

s = tokenizer.decode(idx)
print('decode: ',s)
```

```
encode:  [13000, 13006, 10, 1026, 477, 569, 2947, 2194, 2589, 3578, 354, 2433, 10, 13001]
===================================================================
token:  [b'<|im_start|>', b'user', b'\n', b'\xe7\xbb\x99', b'\xe5\xae\x9a', b'\xe4\xb8\x80\xe4\xb8\xaa', b'\xe8\x8b\xb1\xe6\x96\x87', b'\xe5\x8f\xa5', b'\xe5\xad\x90\xef\xbc\x8c', b'\xe7\xbf\xbb\xe8\xaf\x91', b'\xe6\x88\x90', b'\xe4\xb8\xad\xe6\x96\x87', b'\n', b'<|im_end|>']
===================================================================
text:  ['<|im_start|>', 'user', '\n', 'ç»™', 'å®š', 'ä¸€ä¸ª', 'è‹±æ–‡', 'å¥', 'å­ï¼Œ', 'ç¿»è¯‘', 'æˆ', 'ä¸­æ–‡', '\n', '<|im_end|>']
===================================================================
decode:  <|im_start|>user
ç»™å®šä¸€ä¸ªè‹±æ–‡å¥å­ï¼Œç¿»è¯‘æˆä¸­æ–‡
<|im_end|>
```

## 3.3 Pre-training

Pre-training Dataset Example

```
<|beginoftext|>æ°”å€™å˜åŒ–æŒ‡åœ°çƒæ°”å€™ç³»ç»Ÿçš„é•¿æœŸå˜åŒ–ï¼Œè¿™ç§å˜åŒ–é€šå¸¸æ˜¯ç”±äºäººç±»æ´»åŠ¨æ‰€å¼•èµ·çš„å¤§æ°”ä¸­æ¸©å®¤æ°”ä½“çš„æ’æ”¾è€Œå¯¼è‡´çš„ã€‚å…¶å¯èƒ½çš„å½±å“åŒ…æ‹¬ï¼šæç«¯å¤©æ°”ç°è±¡çš„åŠ å‰§ï¼Œå¦‚æš´é›¨ã€å¹²æ—±ã€æ´ªæ¶ç¾å®³ï¼›æµ·å¹³é¢ä¸Šå‡ï¼Œå¯¼è‡´æµ·å²¸çº¿çš„æ”¹å˜ï¼›å…¨çƒæ°”æ¸©çš„ä¸Šå‡ï¼Œå¯¼è‡´ç”Ÿæ€ç³»ç»Ÿçš„å´©æºƒå’Œç‰©ç§çš„ç­ç»ï¼›ä»¥åŠå…¨çƒç²®é£Ÿå®‰å…¨å’Œèƒ½æºå®‰å…¨çš„é—®é¢˜ç­‰ã€‚<|endoftext|>
```

Before pre-training, encode the dataset and save it in `dataset.bin`:

```
python build_dataset.py
```

Run the pre-training:

```bash
python train.py
```

## 3.4 SFT Fine-tuning

The SFT dataset comes from the contributions of [BELLE](https://github.com/LianjiaTech/BELLE), specifically from: [train_0.5M_CN](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN), which has approximately 500,000 lines.

Example of SFT instruction fine-tuning dataset:

```json
{
    "prompt": "è§£é‡Šä»€ä¹ˆæ˜¯â€œæ°”å€™å˜åŒ–â€ï¼Œå¹¶æ¦‚è¿°å…¶å¯èƒ½çš„å½±å“ã€‚",
    "response": "æ°”å€™å˜åŒ–æŒ‡åœ°çƒæ°”å€™ç³»ç»Ÿçš„é•¿æœŸå˜åŒ–ï¼Œè¿™ç§å˜åŒ–é€šå¸¸æ˜¯ç”±äºäººç±»æ´»åŠ¨æ‰€å¼•èµ·çš„å¤§æ°”ä¸­æ¸©å®¤æ°”ä½“çš„æ’æ”¾è€Œå¯¼è‡´çš„ã€‚å…¶å¯èƒ½çš„å½±å“åŒ…æ‹¬ï¼šæç«¯å¤©æ°”ç°è±¡çš„åŠ å‰§ï¼Œå¦‚æš´é›¨ã€å¹²æ—±ã€æ´ªæ¶ç¾å®³ï¼›æµ·å¹³é¢ä¸Šå‡ï¼Œå¯¼è‡´æµ·å²¸çº¿çš„æ”¹å˜ï¼›å…¨çƒæ°”æ¸©çš„ä¸Šå‡ï¼Œå¯¼è‡´ç”Ÿæ€ç³»ç»Ÿçš„å´©æºƒå’Œç‰©ç§çš„ç­ç»ï¼›ä»¥åŠå…¨çƒç²®é£Ÿå®‰å…¨å’Œèƒ½æºå®‰å…¨çš„é—®é¢˜ç­‰ã€‚"
}
```

```
è®­ç»ƒè¯­æ–™(chatmlæ ¼å¼)ï¼š
<|beginoftext|><|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nè§£é‡Šä»€ä¹ˆæ˜¯â€œæ°”å€™å˜åŒ–â€ï¼Œå¹¶æ¦‚è¿°å…¶å¯èƒ½çš„å½±å“ã€‚<|im_end|>\n<|im_start|>assistant\næ°”å€™å˜åŒ–æŒ‡åœ°çƒæ°”å€™ç³»ç»Ÿçš„é•¿æœŸå˜åŒ–ï¼Œè¿™ç§å˜åŒ–é€šå¸¸æ˜¯ç”±äºäººç±»æ´»åŠ¨æ‰€å¼•èµ·çš„å¤§æ°”ä¸­æ¸©å®¤æ°”ä½“çš„æ’æ”¾è€Œå¯¼è‡´çš„ã€‚å…¶å¯èƒ½çš„å½±å“åŒ…æ‹¬ï¼šæç«¯å¤©æ°”ç°è±¡çš„åŠ å‰§ï¼Œå¦‚æš´é›¨ã€å¹²æ—±ã€æ´ªæ¶ç¾å®³ï¼›æµ·å¹³é¢ä¸Šå‡ï¼Œå¯¼è‡´æµ·å²¸çº¿çš„æ”¹å˜ï¼›å…¨çƒæ°”æ¸©çš„ä¸Šå‡ï¼Œå¯¼è‡´ç”Ÿæ€ç³»ç»Ÿçš„å´©æºƒå’Œç‰©ç§çš„ç­ç»ï¼›ä»¥åŠå…¨çƒç²®é£Ÿå®‰å…¨å’Œèƒ½æºå®‰å…¨çš„é—®é¢˜ç­‰ã€‚<|im_end|><|endoftext|>
```

Run the SFT fine-tuning:
``` bash
python sft.py
```

## 3.5 RLHF (Reinforcement Learning from Human Feedback) â€” To be updated ~

There are two main preference methods, PPO and DPO. Please search for papers and blogs for specific implementations. **This project uses the DPO fine-tuning method, which is comparatively memory-efficient.**

**DPO (Direct Preference Optimization) Fine-tuning** Based on the SFT model, fine-tuning can begin without training a reward model, simply by obtaining positive responses (chosen) and negative responses (rejected).

Example of DPO preference optimization dataset:

```json
{
    "prompt": "è¯·ä»‹ç»ä¸€ä¸‹æµ™æ±Ÿå¤§å­¦",
    "chosen": "æµ™æ±Ÿå¤§å­¦æ˜¯ä¸€æ‰€å†å²æ‚ ä¹…ã€å£°èª‰å“è‘—çš„é«˜ç­‰å­¦åºœï¼Œåè½äºä¸­å›½å†å²æ–‡åŒ–ååŸã€é£æ™¯æ—…æ¸¸èƒœåœ°æ­å·ã€‚",
    "rejected": "æµ™æ±Ÿå¤§å­¦æ˜¯ä¸€æ‰€é‡é¸¡å¤§å­¦ã€‚"
}
```

Run preference optimization: To be updated ~

## 3.6 Inference

Ensure that the `model_save` and `Tokenizer` directories contain the following files:

```bash
mini-ChatGPT-Chinese
â”œâ”€model_save
|  â”œâ”€SFT_GPT.pth
â”œâ”€Tokenizer
|  â””â”€tokenizer.bin
```

Run in the console:

```bash
python client.py
```

# IV.ğŸ“Citation
If you find this project helpful, feel free to cite it.

```conf
@misc{mini-ChatGPT-Chinese,
    author={Jiaxing Song},
    title={mini-ChatGPT-Chinese-0.1B},
    year={2024},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/JiaxingSong718/mini-ChatGPT-Chinese}},
}
```

# V.ğŸ¤”Other Matters
This project does not assume any risks or responsibilities arising from data security, public opinion risks, or any misguidance, misuse, dissemination, or improper use of the open-source model and code.
